{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 and 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would you define Machine Learning?   \n",
    "The process of programming a computer to execute a process with an outcome where through data the computer learns and performs the process with the desired outcome in a more effective manner relative to some performance metric. \n",
    "\n",
    "2. Can you name four types of problems where it shines?   \n",
    "Where is no closed form solution, adaptive programs rather than rule based, reactive programs that adjust for the environment, help gain insights into data for business decisions.\n",
    "\n",
    "3. What is a labeled training set?    \n",
    "A training set where the correct outcome is already specified.  \n",
    "\n",
    "4. What are the two most common supervised tasks?   \n",
    "Classification and Regression\n",
    "\n",
    "5. Can you name four common unsupervised tasks?   \n",
    "Clustering, anomaly detection, dimensionality reduction, association rule learning.\n",
    "\n",
    "6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains?   \n",
    "Reinforcement learning as the terrain may be of any kind and the robot will need to adapt.\n",
    "\n",
    "7. What type of algorithm would you use to segment your customers into multiple groups?   \n",
    "Clustering if one wants to only group customers or a classification algorithm the groups are known \n",
    "\n",
    "8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem?   \n",
    "This would be a supervised learning problem.\n",
    "\n",
    "9. What is an online learning system?   \n",
    "A ml model that receives streaming data and learns from data incrementally and not in batch. Can change quickly to incoming data.\n",
    "\n",
    "10. What is out-of-core learning?   \n",
    "ML program that is online in order to train on data that cannot fit into memory\n",
    "\n",
    "11. What type of learning algorithm relies on a similarity measure to make predictions?   \n",
    "Instance based learning system contains the data known and then compares new instances to the known based upon a similarity metric to make predictions. Examples are k-nearest neighbor and decision trees.\n",
    "\n",
    "12. What is the difference between a model parameter and a learning algorithm’s hyperparameter?   \n",
    "A model parameter represents a feature/paramter of the actual model that is used for prediction based upon new data whereas the hyperparameter determines how the model is trained based upon the algorithm being used. \n",
    "\n",
    "13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?   \n",
    "Search for optimal values for the model parameters that will enable the function to predict new values well. Values are found by approaches that minimize a cost function and potentially apply penalties for complex models. Predictions for new values are made by applying model to new data.  \n",
    "\n",
    "14. Can you name four of the main challenges in Machine Learning?   \n",
    "Relative to data: lack of data, messy data, unrepresentative data; uninformative features; underfitting data due to overly simple model; overfitting data due to overly complex model\n",
    "\n",
    "15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?   \n",
    "This indicates overfitting the model to the training data. Solutions can be: 1. Applying regularization, 2. Employing a simplier model, 3. Get more data for training and/or reducing the noise in the data.\n",
    "\n",
    "16. What is a test set, and why would you want to use it?   \n",
    "A test is a set that is held out of the training set that is used for testing the model is not being overfit. \n",
    "\n",
    "17. What is the purpose of a validation set?   \n",
    "To test different models and hyperparameter tuning. \n",
    "\n",
    "18. What is the train-dev set, when do you need it, and how do you use it?   \n",
    "When there is a suspicion that there is a difference between the training data and the validation and testing data (i.e. the data that will be representative of live, production data) separate out from the training data a train-dev set. Train and test on validation and train-dev set. If model performs poorly on train-dev set, then overfitting is probably occuring. If performs well on train-dev set but not well on validation set, then there is a likely data mismatch between the training data and the representative testing and validation data. Need to make the training data more representative. \n",
    "\n",
    "19. What can go wrong if you tune hyperparameters using the test set?   \n",
    "The model can overfit the data and not generalize well to unknown data. It will most likely show promising results on the test set that are too promising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_data = '/home/jonathan/Projects/LearningReferences/handson-ml2/datasets/housing/housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(hs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "housing_df = pd.read_csv(hs_data)\n",
    "\n",
    "# Create income category for sampling \n",
    "housing_df['income_cat'] = pd.cut(housing_df.median_income, bins=[0, 1.5, 3.0, 4.5, 6.0, np.inf], labels=range(1,6))\n",
    "\n",
    "# Get train/test sample that is representative of income\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43)\n",
    "train_idx, test_idx = next(strat_split.split(housing_df, housing_df.income_cat))\n",
    "hs_train_df = housing_df.loc[train_idx]\n",
    "hs_test_df = housing_df.loc[test_idx]\n",
    "\n",
    "# drop income_cat to put data back to original form\n",
    "hs_train_df.drop(columns='income_cat', inplace=True)\n",
    "hs_test_df.drop(columns='income_cat', inplace=True)\n",
    "\n",
    "hs_train_labels_df = hs_train_df.median_house_value.copy()\n",
    "hs_train_df.drop(columns='median_house_value', inplace=True)\n",
    "\n",
    "# Create custom features from existing data - can also have this as custom transformer to be used in pipeline\n",
    "hs_train_df[\"rooms_per_household\"] = hs_train_df[\"total_rooms\"]/hs_train_df[\"households\"]\n",
    "hs_train_df[\"bedrooms_per_room\"] = hs_train_df[\"total_bedrooms\"]/hs_train_df[\"total_rooms\"]\n",
    "hs_train_df[\"population_per_household\"]=hs_train_df[\"population\"]/hs_train_df[\"households\"]\n",
    "\n",
    "# Replace nan values with median\n",
    "housing_numeric = hs_train_df.drop('ocean_proximity', axis=1)\n",
    "\n",
    "# Deal with categorical data\n",
    "housing_cat = hs_train_df.loc[:, ['ocean_proximity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pipeline\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                         ('std_scaler', StandardScaler())\n",
    "                        ])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([('num', num_pipeline, list(housing_numeric)),\n",
    "                                            ('cat', OneHotEncoder(), list(housing_cat))\n",
    "                                           ])\n",
    "\n",
    "model_pipeline = Pipeline([('preproc', preprocessing_pipeline),\n",
    "                           ('svm', SVR())\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CrossValidation\n",
    "param_grid = [{'svm__kernel': ['linear','rbf', 'sigmoid'],\n",
    "               'svm__C': [.1, .5, 1],\n",
    "               'svm__gamma': ['scale', 'auto']},\n",
    "              {'svm__kernel': ['poly'],\n",
    "               'svm__C': [.1, .5, 1],\n",
    "               'svm__gamma': ['scale', 'auto'],\n",
    "               'svm__degree': [2, 3, 5]}]\n",
    "grid_cv= GridSearchCV(model_pipeline, param_grid, cv=3, return_train_score=True, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.fit(hs_train_df, hs_train_labels_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: (116388.94596328198, {'svm__C': 0.1, 'svm__degree': 2, 'svm__gamma': 'scale', 'svm__kernel': 'linear'})\n",
      "Worst Model: (117293.24626128966, {'svm__C': 1, 'svm__degree': 5, 'svm__gamma': 'auto', 'svm__kernel': 'sigmoid'})\n"
     ]
    }
   ],
   "source": [
    "cv_res = grid_cv.cv_results_\n",
    "cv_res.keys()\n",
    "cv_param_scrs = [(np.sqrt(-score), params) for params, score in zip(cv_res['params'], cv_res['mean_test_score'])]\n",
    "cv_params_scrs = sorted(cv_param_scrs, key=itemgetter(0))\n",
    "print(f'Best Model: {cv_param_scrs[0]}')\n",
    "print(f'Worst Model: {cv_param_scrs[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try replacing GridSearchCV with RandomizedSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__kernel': ['linear', 'poly'],\n",
    "              'svm__C': stats.expon(),\n",
    "              'svm__gamma': ['scale'],\n",
    "              'svm__epsilon': stats.lognorm(1)\n",
    "             }\n",
    "rand_cv= RandomizedSearchCV(model_pipeline, param_grid, cv=3, return_train_score=True, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_cv.fit(hs_train_df, hs_train_labels_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: (108948.09944728023, {'svm__C': 0.5187550357055456, 'svm__epsilon': 1.6838856168104908, 'svm__gamma': 'scale', 'svm__kernel': 'linear'})\n",
      "Worst Model: (116555.2635423261, {'svm__C': 0.09108922219234733, 'svm__epsilon': 0.38185738940253505, 'svm__gamma': 'scale', 'svm__kernel': 'linear'})\n"
     ]
    }
   ],
   "source": [
    "rcv_res = rand_cv.cv_results_\n",
    "rcv_res.keys()\n",
    "rcv_param_scrs = [(np.sqrt(-score), params) for params, score in zip(rcv_res['params'], rcv_res['mean_test_score'])]\n",
    "rcv_params_scrs = sorted(rcv_param_scrs, key=itemgetter(0))\n",
    "print(f'Best Model: {rcv_param_scrs[0]}')\n",
    "print(f'Worst Model: {rcv_param_scrs[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try adding a transformer in the preparation pipeline to select only the most important attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Choose highest correlation features\n",
    "import copy\n",
    "X_clone = dict()\n",
    "class ChooseHighestCorrAttributes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_n_feats=5, is_cat=False):\n",
    "        self.is_cat = is_cat\n",
    "        self.top_n_feats = top_n_feats\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        num_feats = self.top_n_feats - 1\n",
    "        if self.is_cat:\n",
    "            corrs = np.abs([stats.pearsonr(np.squeeze(np.asarray(v)), y)[0] for v in X.T.todense()])\n",
    "        else:\n",
    "            corrs = np.abs([stats.spearmanr(v, y).correlation for v in X.T])\n",
    "        inds = np.argpartition(corrs, -num_feats)\n",
    "        self.top_feat_inds = inds[-num_feats:]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[:, self.top_feat_inds]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pipeline\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                         ('std_scaler', StandardScaler()),\n",
    "                         ('top_corr_feats', ChooseHighestCorrAttributes())\n",
    "                        ])\n",
    "\n",
    "cat_pipeline = Pipeline([('encode', OneHotEncoder()),\n",
    "                         ('top_corr_cfeats', ChooseHighestCorrAttributes(is_cat=True))\n",
    "                        ])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([('num', num_pipeline, list(housing_numeric)),\n",
    "                                            ('cat', cat_pipeline, list(housing_cat))\n",
    "                                           ])\n",
    "\n",
    "model_pipeline = Pipeline([('preproc', preprocessing_pipeline),\n",
    "                           ('svm', SVR())\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CrossValidation\n",
    "param_grid = [{'svm__kernel': ['linear','rbf', 'sigmoid'],\n",
    "               'svm__C': [.1, .5, 1],\n",
    "               'svm__gamma': ['scale', 'auto']},\n",
    "              {'svm__kernel': ['poly'],\n",
    "               'svm__C': [.1, .5, 1],\n",
    "               'svm__gamma': ['scale', 'auto'],\n",
    "               'svm__degree': [2, 3, 5]}]\n",
    "grid_cv = GridSearchCV(model_pipeline, param_grid, cv=3, return_train_score=True, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.fit(hs_train_df, hs_train_labels_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: (117686.99910567568, {'svm__C': 0.1, 'svm__gamma': 'scale', 'svm__kernel': 'linear'})\n",
      "Worst Model: (10991919.088169735, {'svm__C': 1, 'svm__degree': 5, 'svm__gamma': 'auto', 'svm__kernel': 'poly'})\n"
     ]
    }
   ],
   "source": [
    "cv_res = grid_cv.cv_results_\n",
    "cv_res.keys()\n",
    "cv_param_scrs = [(np.sqrt(-score), params) for params, score in zip(cv_res['params'], cv_res['mean_test_score'])]\n",
    "cv_params_scrs = sorted(cv_param_scrs, key=itemgetter(0))\n",
    "print(f'Best Model: {cv_param_scrs[0]}')\n",
    "print(f'Worst Model: {cv_param_scrs[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
