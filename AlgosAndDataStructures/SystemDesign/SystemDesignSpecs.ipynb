{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c0849e-85c8-41df-8da6-193c97a57b33",
   "metadata": {},
   "source": [
    "# System Designs Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8394da5-12d8-41b5-be2b-273cd6ec1513",
   "metadata": {},
   "source": [
    "## Web Service System for Millions of Users\n",
    "![Final Million User System](Resources/FinalWebSrvr.JPG)\n",
    "\n",
    "(One Data Center Shown as Example)\n",
    "* **Load Balaner** \n",
    "    * Sends traffic to correct data center based upon GEO DNS routing\n",
    "    * Distributes traffic envenly to web servers within a data center\n",
    "    * Routes data only healthy web servers or data centers\n",
    "* **CDN - Content Delivery Network**\n",
    "    * Geographically closest server serves content\n",
    "    * Can cache and deliver based upon request path, cookines, query strings, and headers\n",
    "* **Stateless Webs Server Architecture**\n",
    "    * Session state data stored in persisten data store\n",
    "    * Allows for scaling web tier\n",
    "    * Session state store across all data centers to re-route traffic due to outage\n",
    "* **Data Tier follows a Master-Slave Replication design with Sharding**\n",
    "    * Master database receives all writes, updates, and deletes\n",
    "    * All data is copied to slaves which receive all read requests\n",
    "    * Provides high availability and reliability\n",
    "    * Data is sharded across multiple Master-Slave constellations\n",
    "* **Message Queue Follows Pub-Sub Pattern for Task Processing**\n",
    "* **Logging and Metrics Performed**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43f859-a800-44f6-b6d1-54a906d2811a",
   "metadata": {},
   "source": [
    "## Rate Limiter Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742f32b-445a-410c-9ed8-9406bdc2cdf3",
   "metadata": {},
   "source": [
    "![Rate Limiter](Resources/RateLimiter.JPG)\n",
    "\n",
    "* **Rate Limiter Middleware**\n",
    "    * Assuming microservice design, could also use rate limiter functionality in API Gateway\n",
    "    * Uses Leaking Bucket rate limiting Algorithm (See Below)\n",
    "        * Requests that are rate limited receive a HTTP 429 response with appropriate Headers\n",
    "        * e.g. *X-Ratelimit-Remaining*, *X-Ratelimit-Limit*, *X-Ratelimit-Retry-After*\n",
    "* **Counts stored in Cache**\n",
    "* **Rules/Configuration for Rate Limiting Stored on Disk**\n",
    "    * Workers pull from disk to update rules and store in/update cache\n",
    "    * Allows for starting new rate limiters\n",
    "* **Crucial to Have Monitoring of Rate Limiting Metrics** \n",
    "* **Multi-DataCenter or Distributed Environemnt Challenges**\n",
    "    * Race Conditions for updating counters $\\rightarrow$ sorted data sets structure\n",
    "    * Synchronization of data between rate limiters $\\rightarrow$ potentially use centralized data store and/or adopt eventual consistency\n",
    "    \n",
    "### Rate Limiting Algorithms\n",
    "* **Token Bucket**\n",
    "    * Parameters: bucket capactiy of *N* tokens and tokens are added to bucket at rate *R* (how many tokens per unit of time)\n",
    "    * Each request needs a token for processing\n",
    "    * If no tokens available, request is dropped\n",
    "    * Can either have a global bucket or a bucket for each API endpoint\n",
    "* **Leaking Bucket**\n",
    "    * Parameters: FIFO queue size *S* and outflow rate *R* (number of requests to process in a fixed time unit)\n",
    "    * Requests are put on the queue if there is room and processed at a fixed rate\n",
    "    * Otherwise request is dropped\n",
    "* **Fixed Window Counter**\n",
    "    * Parameters: time window size *S* and max counter of *C*\n",
    "    * Timeline is divided into windows of size *S* and each window can process upto *C* requests, with each request incrementing the counter that is initialized at 0\n",
    "    * When counter is full for a window, requests are dropped\n",
    "    * Issue: Can exceed *C* if there is a spike in requests at the edges of windows\n",
    "* **Sliding Window Log**\n",
    "    * Parameters: max request count *C* per unit of time window *T* (e.g. 60 seconds lookback)\n",
    "    * Request timestamp stored in a cache and all requests older than current timestamp minus time window *T* are removed\n",
    "    * If log/cache size is less than max request count *C*, request is processed\n",
    "    * Else, the request is dropped but kept in the cache/log\n",
    "* **Sliding Window Counter**\n",
    "    * Hybrid between Fixed Window Counter and Sliding Window Log algorithms\n",
    "        * Fixed window length *FW*\n",
    "        * Rolling window length *RW*\n",
    "        * Max requests *C*\n",
    "        * Number of requests for a rolling window calculated using:    \n",
    "        REQUESTS_IN_CURR_WINDOW + REQUESTS_IN_PREV_WINDOW * OVERLAP_PERCENT_OF_ROLLING_WINDOW_AND_PREV_WINDOW\n",
    "        * Example: If 3 requests in current window, 7 in previous window, and the rolling window has a 70% overlap with previous window $\\rightarrow 3 + 7\\times .70 = 6.5$ \n",
    "    * Can round up or down the calculated max requests in a rolling window\n",
    "    * Reject or accept requests if number is greater than *C*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c011b5-a02e-49c6-b5fe-9113692e6a3a",
   "metadata": {},
   "source": [
    "## Consistent Hashing Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bdedbb-57d5-42cb-8462-bbfe6996e90e",
   "metadata": {},
   "source": [
    "![HashRing](Resources/HashRing.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ed8b0-363f-48d1-844f-753ee7a7e4b1",
   "metadata": {},
   "source": [
    "* Naive hashing approach is: $hash(\\text{key}) \\% \\text{NUM_SERVERS}$\n",
    "    * Does account for servers being added or removed\n",
    "    * Does not offer much flexibility for custom hashing\n",
    "* Consistent Hashing Solves both\n",
    "    * Basic Consistent Hashing\n",
    "        * Create Hash Ring: If hash function $hash\\_func$ has output domain $hash\\_func \\in [x_0, x_n]$, if you connect the endpoints, you have a hash ring\n",
    "        * Hash servers by IP or name and map them onto Hash Ring\n",
    "        * Hash keys for a request is mapped onto Hash Ring and first server on ring as you progress clockwise is where the requests gets assigned\n",
    "    * Using Virtual nodes allows for more even key distribution and consistent partition sizes for each server\n",
    "        * For each server $s_k$ on the hash ring, create $n$ virtual nodes $s_{k,n}$ that represent the actual server, e.g. $s_{k,1} \\rightarrow s_k$\n",
    "        * These virtual nodes are spread around the hash ring\n",
    "        * When are request is mapped to the ring, the first virtual node going clockwise is the server that request is assigned to\n",
    "* Adding or removing new servers with consistent hashing requires only those keys affected to be redistributed\n",
    "    * Example: If servers $s0, s1, \\& s2$ and new server $s3$ is added between $s2$ and $s0$, only the keys that fell between $(s2,  s0)$ are re-mapped to $s3$\n",
    "    * Similar for removing a server: need to only to look at those keys on the hash ring prior to newly added or removed server and after the last server going counter-clockwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b2f62-2801-42fb-aecf-8ea8c94741a1",
   "metadata": {},
   "source": [
    "## Key-Value Store Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d536cf-f595-4bd8-a066-78d77df4671e",
   "metadata": {},
   "source": [
    "**Overview**  \n",
    "![KeyValueDesign](Resources/KeyValueOverview.JPG)\n",
    "\n",
    "* Clients connect via API for reads and writes through a coordinator node\n",
    "* Nodes are distributed using consistent hashing and data is replicated\n",
    "* All nodes have the same set of responsibilities: Client API, Conflict Resolution, Replication, Failure Detection, Failure Repair, Storage Engine, etc\n",
    "\n",
    "**Write to Specific Node**   \n",
    "![KeyValueDesign](Resources/KeyValueWrite.JPG)\n",
    "\n",
    "* Write persisted to commit log file and saved in cache memory\n",
    "* When cache reaches threshold, data is flushed to disk\n",
    "    * Here a SSTable - sorted-string table   \n",
    "    * Sorted list of key-value pairs      \n",
    "$ $   \n",
    "\n",
    "**Read from a Specific Node**   \n",
    "![KeyValueDesign](Resources/KeyValueRead.JPG)\n",
    "\n",
    "* Check if first in cache and if so return\n",
    "* Checks bloom filter to determine which SSTable may have the data\n",
    "\n",
    "Design leverages:   \n",
    "* CP or AP gurantee, depending on business needs \n",
    "* Eventual consistency with a Vector clock for resolution\n",
    "* The Gossip Protocol with a Sloppy Quorom and Hinted Handoff for node failure handling\n",
    "* Merkle Tree anti-entropy approach for replica syncing for permanent node failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d559f52-46a0-413a-b6c1-c3ea602d8232",
   "metadata": {},
   "source": [
    "## Associated Topics\n",
    "\n",
    "* **CAP Theorem** - \n",
    "    * For distributed systems, a max of 2 of the below features are possible\n",
    "        * **Consistency** - all clients see the same data at the same time no matter which node they connect to\n",
    "        * **Availability** - any client which requests data gets a response even if some nodes are down\n",
    "        * **Partition Tolerance** - the system continues to operate despite network partitions, which is a communication break between two nodes\n",
    "    * Key-Value stores mostly support one of 2 guarantees\n",
    "        * **CP** or **AP**\n",
    "            * If node goes down, to maintain consistency writes are not allowed to remaining nodes so stale data is not returned\n",
    "            * If node goes down, to maintain availability write are allowed, stale data is allowed to be returned, and data will be synced when the down node comes back up \n",
    "        * **CA** are not real-world realistic, since network failures are unavoidable \n",
    "* **Data Replication and Partition Using Consistent Hashing**\n",
    "    * Consistency Models sync data across replicas across multiple nodes\n",
    "        * **Quorom Consensus** \n",
    "            * *N* is the number of nodes, *W* is the write quorom size, and *R* is the read quorom\n",
    "            * For a write to be successful, it must be acknowledged from *W* replicas\n",
    "            * For a read to be successful, it must be responded from *R* replicas\n",
    "            * *W* + *R* > N ensures strong consistency\n",
    "        * **Strong Consistency** - A client never sees out-of-date data\n",
    "        * **Weak Consistency** - Subsequent reads may not see most up-to-date data\n",
    "        * **Eventual Consistency** - Given enough time, all updates are propagated and all replicas consistent\n",
    "    * Client Inconsistency Resolution\n",
    "        * For Eventual Consistency, concurrent writes may cause inconsistent values to be returned and the client must resolve \n",
    "        * **Versioning** - treat each data modification as a new immutable version of data\n",
    "        * **Vector Clock** \n",
    "            * [*server*, *version*] pair assocaited with data\n",
    "            * For writes/updates, a server increments $v_k$ if $[s_i, v_k]$ exists or creates a new entry $[s_i, 1]$\n",
    "            * Clients detect if there is a conflict if all versions in another data item are not less or equal the particular data item\n",
    "            * Can set a parameter that keeps only the *n* most recent pairs\n",
    "* **Failure Detection and Handling**\n",
    "    * Failure Detection\n",
    "        * **All-to-All Multicasting**\n",
    "            * All servers are connected to all servers\n",
    "            * When two servers detect a server is down, it is marked as down\n",
    "            * Inefficient \n",
    "        * **Gossip Protocol**\n",
    "            * Each  node maintains a node membership list of node IDs and heartbeat counters\n",
    "            * Each node periodically increments its heartbeat counter and sends count to a set of random nodes, which in turn propagate to another set of nodes\n",
    "            * Membership list updated when heartbeats are received\n",
    "            * If a member has not been update for a certain amount of time, marked as offline after verified by a certain number of nodes, e.g. 2\n",
    "    * Handling Temporary Failures - After node is detected and marked down\n",
    "        * **Strict Quorom** - read and write operations potentially blocked\n",
    "        * **Sloppy Quorom**\n",
    "            * Improves availability by not strictly enforcing the quorom requirement\n",
    "            * Chooses first *W* healthy serves for writes and first *R* healthy servers for reads on hash ring\n",
    "        * **Hinted Handoff**\n",
    "            * Another server temporarily handles down server's traffic\n",
    "            * When down server is up, changes pushed back to achieve data consistency\n",
    "    * Handling Permanent Failures - Node cannot be recovered\n",
    "        * Employ anti-entropy protocol to keep replicas in sync by updating each replica to the newest version through comparison\n",
    "        * **Merkle Tree** does this efficiently\n",
    "            * A hash tree where every non-leaf node is labeled with a hash of the labels or values of its child nodes (values are in the case where the child nodes are leaves)\n",
    "            * Process:\n",
    "                * Divide key space into *N* buckets\n",
    "                * Hash each key in the a bucket and then hash all hashes of the keys to create a hash for the bucket\n",
    "                * Build tree upward to root by calculating hashes of children\n",
    "            * Replicas are compared first at root and if there is a difference, traverse tree until source of mismatch branch is found\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a93c3c-98de-44d7-9c65-6147f3198652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
